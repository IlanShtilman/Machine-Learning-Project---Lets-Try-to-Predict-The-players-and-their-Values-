# -*- coding: utf-8 -*-
"""

Automatically generated by Colab.


## **1. Regression on the data set**
"""

import json
import os

import kaggle

!mkdir /root/.kaggle/
# Installing the Kaggle package

# TODO: Replace with your actual Kaggle username and API key
# DO NOT commit your actual API key to version control
api_token = {"username":"YOUR_KAGGLE_USERNAME", "key":"YOUR_KAGGLE_API_KEY"}

# creating kaggle.json file with the personal API-Key details
# You can also put this file on your Google Drive

with open('/root/.kaggle/kaggle.json', 'w') as file:
  json.dump(api_token, file)
!chmod 600 /root/.kaggle/kaggle.json

"""Downloading the Football Players Data"""

!mkdir ./datasets/football-players-data
!kaggle datasets download maso0dahmed/football-players-data -p ./datasets/football-players-data
!unzip ./datasets/football-players-data/*.zip  -d ./datasets/football-players-data/

"""Load Football Players Data into Pandas Dataframe."""

import pandas as pd
df = pd.read_csv('./datasets/football-players-data/fifa_players.csv', encoding= 'unicode_escape')
df.head()

"""Convert value_euro to millions for easier interpretation"""

df['value_million_euro'] = df['value_euro'] / 1000000
df.head()

"""Feature Engineering"""

import pandas as pd
import numpy as np

# Categorize players into age brackets and create a new column
df['age_bracket'] = pd.cut(df['age'], bins=[0, 23, 28, 32, 100], labels=[0, 1, 2, 3])

# Calculate the difference between potential and overall rating
df['potential_vs_overall'] = df['potential'] - df['overall_rating']

# Display the updated DataFrame
print(df.head())

"""Select features"""

from sklearn.impute import SimpleImputer

# Define the features for the model, including specific columns and those ending with certain skills
features = ['age_bracket', 'potential_vs_overall', 'overall_rating', 'wage_euro',
            'international_reputation(1-5)', 'release_clause_euro'] + \
           [col for col in df.columns if col.endswith(('crossing', 'finishing', 'heading_accuracy',
                                                      'short_passing', 'volleys', 'dribbling', 'curve',
                                                      'freekick_accuracy', 'long_passing', 'ball_control'))]

# Create the feature matrix X and the target vector y
X = df[features]  # Features for model training
y = df['value_euro']  # Target variable, representing player value in euros

"""Handle NaN values in the target variable"""

# Fill missing values in the target variable 'y' with the median value
y = y.fillna(y.median())

"""Log-transform the target variable after handling NaN values"""

# Apply a log transformation to the target variable 'y' to normalize the data
y = np.log1p(y)

"""Split the data"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""Create a preprocessing pipeline"""

from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

# Identify numeric and categorical features
numeric_features = X.select_dtypes(include=['int64', 'float64']).columns
categorical_features = ['age_bracket']

# Define the preprocessing pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ('num', Pipeline([
            # Impute missing numeric values using the median
            ('imputer', SimpleImputer(strategy='median')),
            # Standardize numeric features
            ('scaler', StandardScaler())
        ]), numeric_features),

        ('cat', Pipeline([
            # Impute missing categorical values with a constant value (-1)
            ('imputer', SimpleImputer(strategy='constant', fill_value=-1)),
        ]), categorical_features)
    ])

"""Create a pipeline with preprocessor and random forest"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import Pipeline

# Create a pipeline that first preprocesses the data and then fits a RandomForestRegressor
rf_pipeline = Pipeline([
    ('preprocessor', preprocessor),  # Preprocessing step
    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))  # RandomForestRegressor model
])

"""Fit the pipeline"""

# Fit the pipeline to the training data
rf_pipeline.fit(X_train, y_train)

"""Make predictions"""

# Make predictions on the test data
y_pred = rf_pipeline.predict(X_test)

"""Evaluate the model"""

from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Calculate the Mean Squared Error (MSE) and Root Mean Squared Error (RMSE)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)  # RMSE is the square root of MSE

# Calculate the R-squared score
r2 = r2_score(y_test, y_pred)

# Print the evaluation metrics
print("Model Evaluation:")
print(f"Root Mean Squared Error (log scale): {rmse:.4f}")
print(f"R-squared Score: {r2:.4f}")

"""Feature importance"""

import pandas as pd

# Extract feature importances from the trained RandomForestRegressor
feature_importance = pd.DataFrame({
    'feature': features,  # Feature names
    'importance': rf_pipeline.named_steps['regressor'].feature_importances_  # Feature importances
})

# Sort features by importance in descending order
feature_importance = feature_importance.sort_values('importance', ascending=False)

# Print the top 10 most important features
print("\nTop 10 Most Important Features:")
print(feature_importance.head(10))

"""Example prediction"""

# Select an example player from the test set
example_player = X_test.iloc[0:1]

# Predict the value for the example player
predicted_value = np.expm1(rf_pipeline.predict(example_player)[0])  # Convert back from log scale

# Get the actual value for the example player
actual_value = np.expm1(y_test.iloc[0])  # Convert back from log scale

# Print the prediction and the actual value
print("\nExample Prediction:")
print(f"Predicted Value: {predicted_value:.2f} euros")
print(f"Actual Value: {actual_value:.2f} euros")

"""Print feature values for the example player"""

# Print the feature values for the example player
print("\nFeature values for the example player:")
for feature, value in example_player.iloc[0].items():
    print(f"{feature}: {value:.2f}")

"""Predict the value_euro using a Knn regressor

"""

# prompt: Predict the value_euro using a Knn regressor

from sklearn.neighbors import KNeighborsRegressor

# Create a pipeline with preprocessor and KNN regressor
knn_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('regressor', KNeighborsRegressor(n_neighbors=5))  # You can adjust n_neighbors
])

# Fit the pipeline
knn_pipeline.fit(X_train, y_train)

# Make predictions
y_pred_knn = knn_pipeline.predict(X_test)

# Evaluate the model
mse_knn = mean_squared_error(y_test, y_pred_knn)
rmse_knn = np.sqrt(mse_knn)
r2_knn = r2_score(y_test, y_pred_knn)

print("\nKNN Model Evaluation:")
print(f"Root Mean Squared Error (log scale): {rmse_knn:.4f}")
print(f"R-squared Score: {r2_knn:.4f}")

"""Example prediction"""

# prompt: Example prediction

# Example prediction for KNN
example_player_knn = X_test.iloc[0:1]
predicted_value_knn = np.expm1(knn_pipeline.predict(example_player_knn)[0])
actual_value_knn = np.expm1(y_test.iloc[0])

print("\nKNN Example Prediction:")
print(f"Predicted Value: {predicted_value_knn:.2f} euros")
print(f"Actual Value: {actual_value_knn:.2f} euros")



"""Prdecit the value_euro with Support Vector Regression (SVR)"""

# prompt: Prdecit the value_euro with Support Vector Regression (SVR)

from sklearn.svm import SVR

# Create a pipeline with preprocessor and SVR
svr_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('regressor', SVR(kernel='rbf'))  # You can try different kernels
])

# Fit the pipeline
svr_pipeline.fit(X_train, y_train)

# Make predictions
y_pred_svr = svr_pipeline.predict(X_test)

# Evaluate the model
mse_svr = mean_squared_error(y_test, y_pred_svr)
rmse_svr = np.sqrt(mse_svr)
r2_svr = r2_score(y_test, y_pred_svr)

print("\nSVR Model Evaluation:")
print(f"Root Mean Squared Error (log scale): {rmse_svr:.4f}")
print(f"R-squared Score: {r2_svr:.4f}")

# Example prediction for SVR
example_player_svr = X_test.iloc[0:1]
predicted_value_svr = np.expm1(svr_pipeline.predict(example_player_svr)[0])
actual_value_svr = np.expm1(y_test.iloc[0])

print("\nSVR Example Prediction:")
print(f"Predicted Value: {predicted_value_svr:.2f} euros")
print(f"Actual Value: {actual_value_svr:.2f} euros")

"""Predict the value_euro with Linear Regression"""

# prompt: Predict the value_euro with Linear Regression

from sklearn.linear_model import LinearRegression

# Create a pipeline with preprocessor and Linear Regression
lr_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('regressor', LinearRegression())
])

# Fit the pipeline
lr_pipeline.fit(X_train, y_train)

# Make predictions
y_pred_lr = lr_pipeline.predict(X_test)

# Evaluate the model
mse_lr = mean_squared_error(y_test, y_pred_lr)
rmse_lr = np.sqrt(mse_lr)
r2_lr = r2_score(y_test, y_pred_lr)

print("\nLinear Regression Model Evaluation:")
print(f"Root Mean Squared Error (log scale): {rmse_lr:.4f}")
print(f"R-squared Score: {r2_lr:.4f}")

# Example prediction for Linear Regression
example_player_lr = X_test.iloc[0:1]
predicted_value_lr = np.expm1(lr_pipeline.predict(example_player_lr)[0])
actual_value_lr = np.expm1(y_test.iloc[0])

print("\nLinear Regression Example Prediction:")
print(f"Predicted Value: {predicted_value_lr:.2f} euros")
print(f"Actual Value: {actual_value_lr:.2f} euros")

"""Predict the value_euro with Decistion Tree"""

# prompt: Predict the value_euro with Decistion Tree

from sklearn.tree import DecisionTreeRegressor

# Create a pipeline with preprocessor and Decision Tree
dt_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('regressor', DecisionTreeRegressor(random_state=42))  # You can adjust hyperparameters
])

# Fit the pipeline
dt_pipeline.fit(X_train, y_train)

# Make predictions
y_pred_dt = dt_pipeline.predict(X_test)

# Evaluate the model
mse_dt = mean_squared_error(y_test, y_pred_dt)
rmse_dt = np.sqrt(mse_dt)
r2_dt = r2_score(y_test, y_pred_dt)

print("\nDecision Tree Model Evaluation:")
print(f"Root Mean Squared Error (log scale): {rmse_dt:.4f}")
print(f"R-squared Score: {r2_dt:.4f}")

# Example prediction for Decision Tree
example_player_dt = X_test.iloc[0:1]
predicted_value_dt = np.expm1(dt_pipeline.predict(example_player_dt)[0])
actual_value_dt = np.expm1(y_test.iloc[0])

print("\nDecision Tree Example Prediction:")
print(f"Predicted Value: {predicted_value_dt:.2f} euros")
print(f"Actual Value: {actual_value_dt:.2f} euros")

"""## **2. Predict value_euro**

Load the data to new variables.
"""

df2 = df.copy()
df2.head()
df2.columns

"""Prepare the data"""

from sklearn.feature_selection import SelectKBest, f_classif

features = ['age', 'overall_rating', 'potential', 'wage_euro', 'crossing',
       'finishing', 'heading_accuracy', 'short_passing', 'volleys',
       'dribbling', 'curve', 'freekick_accuracy', 'long_passing',
       'ball_control', 'acceleration', 'sprint_speed', 'agility', 'reactions',
       'balance', 'shot_power', 'jumping', 'stamina', 'strength', 'long_shots',
       'aggression', 'interceptions', 'positioning', 'vision', 'penalties',
       'composure', 'marking', 'standing_tackle', 'sliding_tackle']

target = ['value_euro']

#Handle missing values
df2 = df.dropna(subset=features + target)

#Categorizing the value_euro
bins = [0, 1e6, 5e6, 10e6, 50e6, np.inf]
labels = ['Very Low', 'Low', 'Medium', 'High', 'Very High']
X2 = df2[features]
y2 = df2[target]
y2_categorial = pd.cut(y2['value_euro'], bins=bins, labels=labels)

#Select the best 10 features to minimize features
selector = SelectKBest(f_classif, k=10)
X2_selected = selector.fit_transform(X2, y2_categorial)
selected_features = X2.columns[selector.get_support()].tolist()

X2_train_selected, X2_test_selected, y2_train, y2_test = train_test_split(X2_selected, y2_categorial, test_size=0.2, random_state=1)

"""Majority Rule foe the labels"""

# prompt: Majority Rule of value_euro

from collections import Counter

# Predict the majority class for each instance in the test set
y_pred_majority = [Counter(y2_train).most_common(1)[0][0]] * len(y2_test)

# Calculate accuracy
accuracy_majority = np.mean(y_pred_majority == y2_test)
print("Majority Rule Accuracy:", accuracy_majority)

"""What the specfic label of Majoritiy"""

# prompt: What the specfic label of Majoritiy and number count of the labels

# Count the occurrences of each label in the training set
label_counts = Counter(y2_train)

# Get the most common label and its count
majority_label, majority_count = label_counts.most_common(1)[0]

print("Majority Label:", majority_label)
print("Majority Count:", majority_count)

# Print the count of each label
for label, count in label_counts.items():
    print(f"Label: {label}, Count: {count}")

"""Create a decision tree classifier using sklearn package and train a model based on the training set."""

from sklearn.tree import DecisionTreeClassifier

clf_tree = DecisionTreeClassifier(max_depth=4, min_samples_leaf=10, random_state=1)
clf_tree.fit(X2_train_selected, y2_train)

"""Draw the tree obtained"""

import matplotlib.pyplot as plt
from sklearn import tree
plt.figure(figsize=(40, 15))
tree.plot_tree(clf_tree, feature_names=selected_features, class_names=clf_tree.classes_, filled=True)
plt.show()

"""Evaluate the preformance of the trained classifier"""

y_pred = clf_tree.predict(X2_test_selected)
y_pred

"""Evaluate the prediction by using accuracy mesaure"""

from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y2_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

"""Checking the classifier for overfitting"""

y_train_pred = clf_tree.predict(X2_train_selected)
train_accuracy = accuracy_score(y2_train, y_train_pred)
print("Training Accuracy: ", train_accuracy)

if train_accuracy - accuracy > 0.1:
  print("The classifier is overfitting")
else:
  print("The classifier is not overfitting")

print(f"Overfitting result: {train_accuracy - accuracy}")

"""Example for player"""

random_index = np.random.randint(0, len(X2_test_selected))
example_player = X2_test_selected[random_index]

example_player_reshaped = example_player.reshape(1, -1)

predicted_category = clf_tree.predict(example_player_reshaped)[0]

print(f"Predicted Value Category for the Example Player: {predicted_category}")

actual_category = y2_test.iloc[random_index]
print(f"Actual Value Category for the Example Player: {actual_category}")

"""Use the KNN to check the accuracy of the training/test"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Initialize and train the KNN classifier with 5 neighbors
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X2_train_selected, y2_train)

# Predict on the test set
y_pred_knn = knn.predict(X2_test_selected)

# Calculate and print the accuracy on the test set
accuracy_knn = accuracy_score(y2_test, y_pred_knn)
print(f"KNN Accuracy: {accuracy_knn:.2f}")

# Predict on the training set
y_train_pred_knn = knn.predict(X2_train_selected)

# Calculate and print the accuracy on the training set
train_accuracy_knn = accuracy_score(y2_train, y_train_pred_knn)
print("KNN Training Accuracy: ", train_accuracy_knn)

# Check for overfitting by comparing training and test accuracy
if train_accuracy_knn - accuracy_knn > 0.1:
    print("The KNN classifier is overfitting")
else:
    print("The KNN classifier is not overfitting")

# Print the difference between training and test accuracy
print(f"Overfitting result: {train_accuracy_knn - accuracy_knn:.2f}")

"""Example for a player"""

# Select a random index from the test set
random_index = np.random.randint(0, len(X2_test_selected))

# Extract the example player's features
example_player = X2_test_selected[random_index]

# Reshape the example player data to be used for prediction
example_player_reshaped = example_player.reshape(1, -1)

# Predict the category for the example player using the KNN model
predicted_category_knn = knn.predict(example_player_reshaped)[0]

# Print the predicted value category
print(f"KNN Predicted Value Category for the Example Player: {predicted_category_knn}")

# Get the actual category for comparison
actual_category = y2_test.iloc[random_index]
print(f"Actual Value Category for the Example Player: {actual_category}")

"""Use the SVM to check the accuracy of the training and test"""

from sklearn.svm import LinearSVC
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score

# Combine the labels from the training and test sets and fit the LabelEncoder
all_labels = np.unique(np.concatenate((y2_train, y2_test)))
le = LabelEncoder()
le.fit(all_labels)

# Encode the target variables (train and test) to integer labels
y_train_encoded = le.transform(y2_train)
y_test_encoded = le.transform(y2_test)

# Initialize and train the Linear SVM classifier
svm = LinearSVC(random_state=42, max_iter=5000)
svm.fit(X2_train_selected, y_train_encoded)

# Predict on the test set and decode the labels
y_pred_encoded = svm.predict(X2_test_selected)
y_pred = le.inverse_transform(y_pred_encoded)

# Calculate and print the test accuracy
test_accuracy = accuracy_score(y2_test, y_pred)
print(f"SVM Test Accuracy: {test_accuracy:.4f}")

# Predict on the training set and decode the labels
y_train_pred_encoded = svm.predict(X2_train_selected)
y_train_pred = le.inverse_transform(y_train_pred_encoded)

# Calculate and print the training accuracy
train_accuracy = accuracy_score(y2_train, y_train_pred)
print(f"SVM Training Accuracy: {train_accuracy:.4f}")

# Check for overfitting by comparing training and test accuracy
if train_accuracy - test_accuracy > 0.1:
    print("The SVM classifier might be overfitting.")
else:
    print("The SVM classifier does not show signs of significant overfitting.")

# Print the difference between training and test accuracy
print(f"Overfitting result: {train_accuracy - test_accuracy:.4f}")

"""Example player"""

random_index = np.random.randint(0, len(X2_test_selected))
example_player = X2_test_selected[random_index]

example_player_reshaped = example_player.reshape(1, -1)

predicted_category_encoded = svm.predict(example_player_reshaped)[0]
predicted_category = le.inverse_transform([predicted_category_encoded])[0]

print(f"SVM Predicted Value Category for the Example Player: {predicted_category}")

actual_category = y2_test.iloc[random_index]
print(f"Actual Value Category for the Example Player: {actual_category}")

"""Use XGBoost to predict value_euros and check the accuracy"""

import xgboost as xgb

# Encode the target variables using LabelEncoder
le = LabelEncoder()
y2_train_encoded = le.fit_transform(y2_train)
y2_test_encoded = le.transform(y2_test)

# Initialize and train the XGBoost model for multi-class classification
xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=len(le.classes_), random_state=42)
xgb_model.fit(X2_train_selected, y2_train_encoded)

# Predict on the test set and decode the labels
y_pred_encoded = xgb_model.predict(X2_test_selected)
y_pred = le.inverse_transform(y_pred_encoded)

# Calculate and print the accuracy on the test set
accuracy = accuracy_score(y2_test, y_pred)
print(f"XGBoost Accuracy: {accuracy:.2f}")

# Predict on the training set and decode the labels
y_train_pred_encoded = xgb_model.predict(X2_train_selected)
y_train_pred = le.inverse_transform(y_train_pred_encoded)

# Calculate and print the training accuracy
train_accuracy = accuracy_score(y2_train, y_train_pred)
print("XGBoost Training Accuracy: ", train_accuracy)

# Check for overfitting by comparing training and test accuracy
if train_accuracy - accuracy > 0.1:
    print("The XGBoost classifier might be overfitting.")
else:
    print("The XGBoost classifier does not show signs of significant overfitting.")

# Print the difference between training and test accuracy
print(f"Overfitting result: {train_accuracy - accuracy:.4f}")

"""Player example"""

random_index = np.random.randint(0, len(X2_test_selected))
example_player = X2_test_selected[random_index]

example_player_reshaped = example_player.reshape(1, -1)

predicted_category_encoded = xgb_model.predict(example_player_reshaped)[0]
predicted_category = le.inverse_transform([predicted_category_encoded])[0]

print(f"XGBoost Predicted Value Category for the Example Player: {predicted_category}")

actual_category = y2_test.iloc[random_index]
print(f"Actual Value Category for the Example Player: {actual_category}")

"""Use Naive Bayes to predict the value_euro (m-estimate)"""

from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score

# Create Gaussian Naive Bayes classifier
gnb = GaussianNB()

# Perform 5-fold cross-validation
cv_scores = cross_val_score(gnb, X, y, cv=5)

# Print the cross-validation scores
print("Cross-validation scores:", cv_scores)

# Calculate and print the average accuracy
average_accuracy = cv_scores.mean()
print("Average accuracy:", average_accuracy)

# Fit the model on the training data
gnb.fit(X2_test_selected, y2_test)

# Predict on the test set
y_pred2 = gnb.predict(X2_test_selected)

# Calculate accuracy
accuracy = accuracy_score(y2_test, y_pred2)
print("Accuracy:", accuracy)

"""## 3. **Predict wage_euros**"""

def create_unique_bins(data, n_bins=6):
    """Create unique bin edges based on percentiles."""
    percentiles = np.linspace(0, 100, n_bins + 1)
    bins = np.unique(np.percentile(data, percentiles))

    # Ensure the last bin edge is infinity
    if bins[-1] != np.inf:
        bins = np.append(bins, np.inf)

    return bins

"""Load the data to variables"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.metrics import accuracy_score
from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder

df3 = df.copy()
df3.head()

"""Selecting features"""

features = ['age', 'overall_rating', 'potential', 'value_euro', 'crossing',
       'finishing', 'heading_accuracy', 'short_passing', 'volleys',
       'dribbling', 'curve', 'freekick_accuracy', 'long_passing',
       'ball_control', 'acceleration', 'sprint_speed', 'agility', 'reactions',
       'balance', 'shot_power', 'jumping', 'stamina', 'strength', 'long_shots',
       'aggression', 'interceptions', 'positioning', 'vision', 'penalties',
       'composure', 'marking', 'standing_tackle', 'sliding_tackle']

target = 'wage_euro'

"""Handle missing values"""

df3 = df3.dropna(subset=features + [target])

"""Categorizing the wage_euro"""

bins = [0, 10000, 50000, 100000, 200000, np.inf]
labels = ['Very Low', 'Low', 'Medium', 'High', 'Very High']
X = df3[features]
y = pd.cut(df3[target], bins=bins, labels=labels)

"""Majority Rule of wage_euro"""

# prompt: Majority Rule of wage_euro

# Assuming y is a Pandas Series with categorical labels for wage_euro
majority_class_wage = Counter(y).most_common(1)[0][0]
majority_class_count_wage = Counter(y).most_common(1)[0][1]

print("Majority Class (Wage Euro Label):", majority_class_wage)
print("Count of Majority Class:", majority_class_count_wage)

"""Majority Rule of wage_euro labels"""

# prompt: Majority Rule of wage_euro labels

# Calculate the accuracy of the majority rule classifier for wage_euro
majority_rule_accuracy_wage = majority_class_count_wage / len(y)
print("Accuracy of Majority Rule Classifier (Wage Euro):", majority_rule_accuracy_wage)

"""Select the best 20 features to minimize features"""

selector = SelectKBest(f_classif, k=20)
X_selected = selector.fit_transform(X, y)
selected_features = X.columns[selector.get_support()].tolist()

"""Split the data"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

"""Encode the labels"""

# Initialize the LabelEncoder
le = LabelEncoder()

# Fit the LabelEncoder on the training labels and transform them
y_train_encoded = le.fit_transform(y_train)

# Transform the test labels using the fitted LabelEncoder
y_test_encoded = le.transform(y_test)

"""Train the XGBoost Classifier"""

# Initialize the XGBoost model with specified hyperparameters
xgb_model = XGBClassifier(
    max_depth=6,              # Maximum depth of the trees
    learning_rate=0.05,       # Learning rate (step size shrinkage)
    n_estimators=200,         # Number of boosting rounds (trees)
    min_child_weight=3,       # Minimum sum of instance weight (hessian) needed in a child
    subsample=0.8,            # Subsample ratio of the training instance
    colsample_bytree=0.8,     # Subsample ratio of columns when constructing each tree
    random_state=1            # Seed for reproducibility
)

# Train the model on the training data
xgb_model.fit(X_train, y_train_encoded)

"""Make predictions"""

y_pred_encoded = xgb_model.predict(X_test)
y_pred = le.inverse_transform(y_pred_encoded)
y_pred

"""Evaluate the model"""

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")

"""Check for overfitting"""

y_train_pred_encoded = xgb_model.predict(X_train)
y_train_pred = le.inverse_transform(y_train_pred_encoded)
train_accuracy = accuracy_score(y_train, y_train_pred)
print(f"Training Accuracy: {train_accuracy:.4f}")

if train_accuracy - accuracy > 0.1:
    print("The classifier is overfitting")
else:
    print("The classifier is not overfitting")

print(f"Overfitting result: {train_accuracy - accuracy}")

"""Player example"""

random_index = np.random.randint(0, len(X_test))
example_player = X_test.iloc[random_index]

example_player_array = example_player.values.reshape(1, -1)  # Reshape for prediction

predicted_category_encoded = xgb_model.predict(example_player_array)[0]
predicted_category = le.inverse_transform([predicted_category_encoded])[0]

print(f"XGBoost Predicted Wage Category for the Example Player: {predicted_category}")

actual_category = y_test.iloc[random_index]
print(f"Actual Wage Category for the Example Player: {actual_category}")

"""Use the KNN to check the accuracy of the training/Test"""

from sklearn.neighbors import KNeighborsClassifier

# Initialize the KNN classifier with 5 neighbors
knn = KNeighborsClassifier(n_neighbors=5)

# Train the KNN classifier on the training data
knn.fit(X_train, y_train_encoded)

# Predict the encoded labels for the test set
y_pred_encoded = knn.predict(X_test)

# Decode the predicted labels back to their original form
y_pred = le.inverse_transform(y_pred_encoded)

# Calculate the accuracy on the test set
accuracy = accuracy_score(y_test, y_pred)
print(f"KNN Accuracy: {accuracy:.4f}")

# Predict the encoded labels for the training set
y_train_pred_encoded = knn.predict(X_train)

# Decode the predicted labels for the training set
y_train_pred = le.inverse_transform(y_train_pred_encoded)

# Calculate the accuracy on the training set
train_accuracy = accuracy_score(y_train, y_train_pred)
print(f"KNN Training Accuracy: {train_accuracy:.4f}")

# Check for overfitting by comparing the training and test accuracies
if train_accuracy - accuracy > 0.1:
    print("The KNN classifier is overfitting")
else:
    print("The KNN classifier is not overfitting")

# Print the difference between training and test accuracy
print(f"Overfitting result: {train_accuracy - accuracy:.4f}")

"""Player example"""

random_index = np.random.randint(0, len(X_test))
example_player = X_test.iloc[random_index]

example_player_array = example_player.values.reshape(1, -1)  # Reshape for prediction

predicted_category_encoded = knn.predict(example_player_array)[0]
predicted_category = le.inverse_transform([predicted_category_encoded])[0]

print(f"KNN Predicted Wage Category for the Example Player: {predicted_category}")

actual_category = y_test.iloc[random_index]
print(f"Actual Wage Category for the Example Player: {actual_category}")

"""Use the SVM to check the accuracy of the training and test"""

from sklearn.svm import LinearSVC

# Initialize the SVM classifier with a maximum of 1000 iterations
svm = LinearSVC(random_state=42, max_iter=1000)

# Train the SVM classifier on the training data
svm.fit(X_train, y_train_encoded)

# Predict the encoded labels for the test set
y_pred_encoded = svm.predict(X_test)

# Decode the predicted labels back to their original form
y_pred = le.inverse_transform(y_pred_encoded)

# Calculate the accuracy on the test set
accuracy = accuracy_score(y_test, y_pred)
print(f"SVM Test Accuracy: {accuracy:.4f}")

# Predict the encoded labels for the training set
y_train_pred_encoded = svm.predict(X_train)

# Decode the predicted labels for the training set
y_train_pred = le.inverse_transform(y_train_pred_encoded)

# Calculate the accuracy on the training set
train_accuracy = accuracy_score(y_train, y_train_pred)
print(f"SVM Training Accuracy: {train_accuracy:.4f}")

# Check for overfitting by comparing the training and test accuracies
if train_accuracy - accuracy > 0.1:
    print("The SVM classifier might be overfitting.")
else:
    print("The SVM classifier does not show signs of significant overfitting.")

# Print the difference between training and test accuracy
print(f"Overfitting result: {train_accuracy - accuracy:.4f}")

"""Player example"""

random_index = np.random.randint(0, len(X_test))
example_player = X_test.iloc[random_index]

example_player_array = example_player.values.reshape(1, -1)  # Reshape for prediction

predicted_category_encoded = svm.predict(example_player_array)[0]
predicted_category = le.inverse_transform([predicted_category_encoded])[0]

print(f"SVM Predicted Wage Category for the Example Player: {predicted_category}")

actual_category = y_test.iloc[random_index]
print(f"Actual Wage Category for the Example Player: {actual_category}")

"""Create a decision tree classifier using sklearn package and train a model based on the training set."""

X_wage_tree = df2[features]
y_wage_tree = df2[target]
y2_categorial = pd.cut(df2['wage_euro'], bins=bins, labels=labels)

#Select the best 10 features to minimize features
selector = SelectKBest(f_classif, k=10)
X2_selected_wage_tree = selector.fit_transform(X_wage_tree, y2_categorial)
selected_features = X_wage_tree.columns[selector.get_support()].tolist()

X2_train_selected_tree, X2_test_selected_tree, y2_train_tree, y2_test_tree = train_test_split(X2_selected_wage_tree, y2_categorial, test_size=0.2, random_state=1)

clf_tree = DecisionTreeClassifier(max_depth=4, min_samples_leaf=10, random_state=1)
clf_tree.fit(X2_train_selected_tree, y2_train_tree)
plt.figure(figsize=(40, 15))
tree.plot_tree(clf_tree, feature_names=selected_features, class_names=clf_tree.classes_, filled=True)
plt.show()

y_pred = clf_tree.predict(X2_test_selected_tree)
accuracy = accuracy_score(y2_test_tree, y_pred)
print(f"Accuracy: {accuracy:.2f}")

y_train_pred = clf_tree.predict(X2_train_selected_tree)
train_accuracy = accuracy_score(y2_train_tree, y_train_pred)
print("Training Accuracy: ", train_accuracy)

if train_accuracy - accuracy > 0.1:
  print("The classifier is overfitting")
else:
  print("The classifier is not overfitting")

print(f"Overfitting result: {train_accuracy - accuracy}")

"""Player example"""

random_index = np.random.randint(0, len(X2_test_selected))
example_player = X2_test_selected[random_index]

example_player_reshaped = example_player.reshape(1, -1)

predicted_category = clf_tree.predict(example_player_reshaped)[0]

print(f"Predicted Value Category for the Example Player: {predicted_category}")

actual_category = y2_test.iloc[random_index]
print(f"Actual Value Category for the Example Player: {actual_category}")

"""Use Naive Bayes to predict the value_euro (m-estimate)"""

# Create Gaussian Naive Bayes classifier
gnb = GaussianNB()

# Perform 5-fold cross-validation
cv_scores = cross_val_score(gnb, X, y, cv=5)

# Print the cross-validation scores
print("Cross-validation scores:", cv_scores)

# Calculate and print the average accuracy
average_accuracy = cv_scores.mean()
print("Average accuracy:", average_accuracy)

# Fit the model on the training data
gnb.fit(X_train, y_train)

# Predict on the test set
y_pred2 = gnb.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred2)
print("Accuracy:", accuracy)